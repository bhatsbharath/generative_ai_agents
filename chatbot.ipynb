{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhatsbharath/generative_ai_agents/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot / Standalone Agent\n",
        "\n",
        "Defines chat_with_ollama to send a prompt to a local Ollama model API (llama3) and return the generated response. Uses the requests library to make a POST request to http://localhost:11434/api/generate. If successful, it returns the generated text; otherwise, an error message."
      ],
      "metadata": {
        "id": "bkyYbg51qjqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def chat_with_ollama(prompt, model=\"llama3\"):\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False  # stream = True if you want to process partial responses\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['response']\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "# Example usage\n",
        "response = chat_with_ollama(\"What is multi agent system in generative ai.\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "Q5KcLqE1xkyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function generate_response that sends a prompt to a local language model API (llama3) and gets a generated text response.\n",
        "\n",
        "It uses the requests library to POST the prompt with parameters like model and temperature.\n",
        "\n",
        "If successful, it returns the generated text; otherwise, it shows an error.\n",
        "\n",
        "The script tests two temperature values (0.2 and 1.2) to show how temperature affects creativity:\n",
        "\n",
        "Lower temperature → more predictable responses.\n",
        "\n",
        "Higher temperature → more creative and varied responses."
      ],
      "metadata": {
        "id": "2Rx2WXrSycdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def generate_response(prompt, model=\"llama3\", temperature=0.7):\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"temperature\": temperature,\n",
        "        \"stream\": False\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()[\"response\"]\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "\n",
        "prompt = \"Write a short sci-fi story in 4-5 sentences about a robot that becomes self-aware..\"\n",
        "\n",
        "# Try different temperatures\n",
        "for temp in [0.2, 1.2]:\n",
        "    print(f\"\\n--- Temperature: {temp} ---\")\n",
        "    print(generate_response(prompt, temperature=temp))\n",
        "\n"
      ],
      "metadata": {
        "id": "_xidb09CyZz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bGctkQx8qxGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Vision Language Model"
      ],
      "metadata": {
        "id": "Fhi9ZWK4q0do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "response = ollama.chat(\n",
        "    model='llama3.2-vision',\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'What is in this image?',\n",
        "        'images': ['TestImage.png']\n",
        "    }]\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "2AG0bOIlzMmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting"
      ],
      "metadata": {
        "id": "whDf4C00q7nA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Zero-Shot Prompting\n",
        "\n",
        "The model is asked to perform a task without any examples. It must rely solely on its prior knowledge."
      ],
      "metadata": {
        "id": "qVMHhFkbzcBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-Shot Prompting\n",
        "prompt = \"Translate 'I like pizza' into French.\"\n",
        "print(generate_response(prompt))"
      ],
      "metadata": {
        "id": "X6HYomimy3BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. One-Shot Prompting\n",
        "\n",
        "The model is given one example of the task before being asked to perform it."
      ],
      "metadata": {
        "id": "yntr12R-zvpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Shot Prompting\n",
        "prompt = \"\"\"Translate the following sentences into French:\n",
        "\n",
        "English: Good morning\n",
        "French: Bonjour\n",
        "\n",
        "English: I like pizza\n",
        "French:\"\"\"\n",
        "print(generate_response(prompt))\n"
      ],
      "metadata": {
        "id": "nRPF5hJay7XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Few-Shot Prompting\n",
        "\n",
        "Few-Shot prompting gives the model multiple examples of a task before asking it to complete a similar one. This helps the model better understand the pattern and produce more accurate results."
      ],
      "metadata": {
        "id": "4QHSIgjGzyhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Few shot prompting\n",
        "prompt = \"\"\"Translate the following sentences into French:\n",
        "\n",
        "English: Good morning\n",
        "French: Bonjour\n",
        "\n",
        "English: How are you?\n",
        "French: Comment ça va ?\n",
        "\n",
        "English: I like pizza\n",
        "French:\"\"\"\n",
        "print(generate_response(prompt))\n"
      ],
      "metadata": {
        "id": "84_XjPe1zJAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Chain-of-Thought Prompting\n",
        "\n",
        "This technique encourages the model to think step-by-step by including reasoning or intermediate steps in the prompt, improving performance on complex tasks. It’s often done by adding explanations or guiding the model through a thought process."
      ],
      "metadata": {
        "id": "HiboXUbEz0-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOllama(\n",
        "\tmodel=\"llama3.2\",\n",
        "\ttemperature=0.7,\n",
        "\ttop_p=0.9,\n",
        "\tnum_predict=256,\n",
        "\trepeat_penalty=1.1\n",
        ")\n",
        "\n",
        "# Standard prompt\n",
        "standard_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Answer the following question concisely: {question}.\"\n",
        ")\n",
        "\n",
        "# Chain of Thought prompt\n",
        "cot_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Answer the following question step by step concisely: {question}\"\n",
        ")\n",
        "\n",
        "# Create chains\n",
        "standard_chain = standard_prompt | llm\n",
        "cot_chain = cot_prompt | llm\n",
        "\n",
        "# Example question\n",
        "question = \"If a train travels 120 km in 2 hours, what is its average speed in km/h?\"\n",
        "\n",
        "# Get responses\n",
        "standard_response = standard_chain.invoke(question).content\n",
        "cot_response = cot_chain.invoke(question).content\n",
        "\n",
        "print(\"Standard Response:\")\n",
        "print(standard_response)\n",
        "print(\"\\nChain of Thought Response:\")\n",
        "print(cot_response)"
      ],
      "metadata": {
        "id": "5MykwBbJzQcQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}