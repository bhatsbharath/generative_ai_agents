{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhatsbharath/generative_ai_agents/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatBOT\n",
        "\n",
        "This code defines a function chat_with_ollama that sends a prompt to a local Ollama language model API (llama3) and returns the generated response.\n",
        "\n",
        "It uses the requests library to POST the prompt to http://localhost:11434/api/generate.\n",
        "\n",
        "The stream parameter is set to False to get the full response at once.\n",
        "\n",
        "If the request is successful, it returns the generated text; otherwise, it returns an error message.\n",
        "\n",
        "The example shows how to ask the model a question about multi-agent systems in generative AI."
      ],
      "metadata": {
        "id": "tZRiE9i6youP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def chat_with_ollama(prompt, model=\"llama3\"):\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False  # stream = True if you want to process partial responses\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['response']\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "# Example usage\n",
        "response = chat_with_ollama(\"What is multi agent system in generative ai.\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "Q5KcLqE1xkyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function generate_response that sends a prompt to a local language model API (llama3) and gets a generated text response.\n",
        "\n",
        "It uses the requests library to POST the prompt with parameters like model and temperature.\n",
        "\n",
        "If successful, it returns the generated text; otherwise, it shows an error.\n",
        "\n",
        "The script tests two temperature values (0.2 and 1.2) to show how temperature affects creativity:\n",
        "\n",
        "Lower temperature → more predictable responses.\n",
        "\n",
        "Higher temperature → more creative and varied responses."
      ],
      "metadata": {
        "id": "2Rx2WXrSycdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def generate_response(prompt, model=\"llama3\", temperature=0.7):\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"temperature\": temperature,\n",
        "        \"stream\": False\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()[\"response\"]\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "\n",
        "prompt = \"Write a short sci-fi story in 4-5 sentences about a robot that becomes self-aware..\"\n",
        "\n",
        "# Try different temperatures\n",
        "for temp in [0.2, 1.2]:\n",
        "    print(f\"\\n--- Temperature: {temp} ---\")\n",
        "    print(generate_response(prompt, temperature=temp))\n",
        "\n",
        "# temperature=0.2: more predictable, safe names like “AIVA” or “NeoAI”.\n",
        "# temperature=1.2: more creative, unusual names like “QuantiNova” or “MindPhantom”."
      ],
      "metadata": {
        "id": "_xidb09CyZz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-Shot Prompting\n",
        "prompt = \"Translate 'I like pizza' into French.\"\n",
        "print(generate_response(prompt))"
      ],
      "metadata": {
        "id": "X6HYomimy3BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q1IEfJXOy5_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Shot Prompting\n",
        "prompt = \"\"\"Translate the following sentences into French:\n",
        "\n",
        "English: Good morning\n",
        "French: Bonjour\n",
        "\n",
        "English: I like pizza\n",
        "French:\"\"\"\n",
        "print(generate_response(prompt))\n"
      ],
      "metadata": {
        "id": "nRPF5hJay7XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Few shot prompting\n",
        "prompt = \"\"\"Translate the following sentences into French:\n",
        "\n",
        "English: Good morning\n",
        "French: Bonjour\n",
        "\n",
        "English: How are you?\n",
        "French: Comment ça va ?\n",
        "\n",
        "English: I like pizza\n",
        "French:\"\"\"\n",
        "print(generate_response(prompt))\n"
      ],
      "metadata": {
        "id": "84_XjPe1zJAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "response = ollama.chat(\n",
        "    model='llama3.2-vision',\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'What is in this image?',\n",
        "        'images': ['TestImage.png']\n",
        "    }]\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "2AG0bOIlzMmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOllama(\n",
        "\tmodel=\"llama3.2\",\n",
        "\ttemperature=0.7,\n",
        "\ttop_p=0.9,\n",
        "\tnum_predict=256,\n",
        "\trepeat_penalty=1.1\n",
        ")\n",
        "\n",
        "# Standard prompt\n",
        "standard_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Answer the following question concisely: {question}.\"\n",
        ")\n",
        "\n",
        "# Chain of Thought prompt\n",
        "cot_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Answer the following question step by step concisely: {question}\"\n",
        ")\n",
        "\n",
        "# Create chains\n",
        "standard_chain = standard_prompt | llm\n",
        "cot_chain = cot_prompt | llm\n",
        "\n",
        "# Example question\n",
        "question = \"If a train travels 120 km in 2 hours, what is its average speed in km/h?\"\n",
        "\n",
        "# Get responses\n",
        "standard_response = standard_chain.invoke(question).content\n",
        "cot_response = cot_chain.invoke(question).content\n",
        "\n",
        "print(\"Standard Response:\")\n",
        "print(standard_response)\n",
        "print(\"\\nChain of Thought Response:\")\n",
        "print(cot_response)"
      ],
      "metadata": {
        "id": "5MykwBbJzQcQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}