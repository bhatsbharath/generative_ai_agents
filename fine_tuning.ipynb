{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPvUB6WzdFxQd6xjRBviBKM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhatsbharath/generative_ai_agents/blob/main/fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA (Low-Rank Adaptation of Large Language Models) fine-tuning\n",
        "Text classification model using the Stanford Sentiment Treebank (SST-2) dataset and LoRA (Low-Rank Adaptation of Large Language Models) fine-tuning a LLM"
      ],
      "metadata": {
        "id": "5ZNt2PjdhQik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj8mqus53dO2"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer)\n",
        "\n",
        "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
        "import evaluate\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sst2\n",
        "# The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. It uses the two-way (positive/negative) class split, with only sentence-level labels.\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "NtdTPNtw3uwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display % of training data with label=1\n",
        "np.array(dataset['train']['label']).sum()/len(dataset['train']['label'])"
      ],
      "metadata": {
        "id": "6pA2DnXl3tH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines label mappings for binary classification (Negative/Positive).\n",
        "\n",
        "Loads a sequence classification model from a pretrained checkpoint configured with these labels."
      ],
      "metadata": {
        "id": "cxsB5En8mCBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = 'roberta-base'\n",
        "\n",
        "# define label maps\n",
        "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
        "label2id = {\"Negative\":0, \"Positive\":1}\n",
        "\n",
        "# generate classification model from model_checkpoint\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
      ],
      "metadata": {
        "id": "QOy-gn2P3yTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display architecture\n",
        "model"
      ],
      "metadata": {
        "id": "591nTe1r31C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads a tokenizer from the pretrained checkpoint.\n",
        "\n",
        "Adds a padding token if missing and resizes the modelâ€™s embeddings accordingly."
      ],
      "metadata": {
        "id": "EURpC5FlmGk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
        "\n",
        "# add pad token if none exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "93YK9v6O32HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines a function to tokenize input texts with left-side truncation and max length 512, returning NumPy tensors."
      ],
      "metadata": {
        "id": "JhCeGC5GmNOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenize function\n",
        "def tokenize_function(examples):\n",
        "    # extract text\n",
        "    text = examples[\"sentence\"]\n",
        "\n",
        "    #tokenize and truncate text\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "iV1prWtW33GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize training and validation datasets\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "xmZ7yVTl34DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "ShwM4vo935BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import accuracy evaluation metric\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "Jv4gWgiv36QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define an evaluation function to pass into trainer later\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
      ],
      "metadata": {
        "id": "M91H1sJ737Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Summary:\n",
        "\n",
        "Defines a list of example sentences.\n",
        "\n",
        "Tokenizes each sentence, feeds it into the model, and gets predicted labels.\n",
        "\n",
        "Prints the sentences with their predicted sentiment labels (Negative/Positive)."
      ],
      "metadata": {
        "id": "MmVoBuSfmWkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define list of examples\n",
        "text_list = [\"a feel-good picture in the best sense of the term .\", \"resourceful and ingenious entertainment .\", \"it 's just incredibly dull .\", \"the movie 's biggest offense is its complete and utter lack of tension .\",\n",
        "             \"impresses you with its open-endedness and surprises .\", \"unless you are in dire need of a diesel fix , there is no real reason to see it .\"]\n",
        "\n",
        "print(\"Untrained model predictions:\")\n",
        "print(\"----------------------------\")\n",
        "for text in text_list:\n",
        "    # tokenize text\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    # compute logits\n",
        "    logits = model(inputs).logits\n",
        "    # convert logits to label\n",
        "    predictions = torch.argmax(logits)\n",
        "\n",
        "    print(text + \" - \" + id2label[predictions.tolist()])"
      ],
      "metadata": {
        "id": "CTY6DwQI38Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configures LoRA parameters for sequence classification with specified rank, alpha, dropout, and target modules."
      ],
      "metadata": {
        "id": "5c9MzszdmZu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
        "                        r=4,\n",
        "                        lora_alpha=32,\n",
        "                        lora_dropout=0.01,\n",
        "                        target_modules = ['query'])"
      ],
      "metadata": {
        "id": "huwL20GT382O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config"
      ],
      "metadata": {
        "id": "v7IXZYLM395-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applies the LoRA configuration to the base model for parameter-efficient fine-tuning.\n",
        "\n",
        "Prints the number of trainable parameters in the adapted model."
      ],
      "metadata": {
        "id": "skhIQE-5mgke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "p-LX1Y6O3_Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "lr = 1e-3\n",
        "batch_size = 16\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "CeQ9BIW74AGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sets training arguments for fine-tuning, including output directory, learning rate, batch size, epochs, weight decay, and evaluation/save strategies."
      ],
      "metadata": {
        "id": "_6dqKcEimkss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= model_checkpoint + \"-lora-text-classification\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "9eu-UKwM4BHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a Trainer object with model, training args, datasets, tokenizer, and evaluation setup.\n",
        "\n",
        "Starts fine-tuning the model by calling trainer.train()."
      ],
      "metadata": {
        "id": "H9dpyMlbmp7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creater trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "kADNcijh4CKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moves the trained model to CPU.\n",
        "\n",
        "Runs predictions on example texts using the fine-tuned model.\n",
        "\n",
        "Prints each text with its predicted sentiment label."
      ],
      "metadata": {
        "id": "AAuj92eemusG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cpu')\n",
        "\n",
        "print(\"Trained model predictions:\")\n",
        "print(\"--------------------------\")\n",
        "for text in text_list:\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "    logits = model(inputs).logits\n",
        "    predictions = torch.max(logits,1).indices\n",
        "\n",
        "    print(text + \" - \" + id2label[predictions.tolist()[0]])"
      ],
      "metadata": {
        "id": "fl1angTx4DL-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}